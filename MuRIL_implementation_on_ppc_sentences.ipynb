{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20831673-5bba-44fe-b633-161a98adfd92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **Tensorflow prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34645ab9-aa4d-4d17-a74b-3221e0905634",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-for-tf2\n",
      "  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n",
      "     -------------------------------------- 41.2/41.2 kB 657.8 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py-params>=0.9.6\n",
      "  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting params-flow>=0.8.0\n",
      "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.21.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from tqdm->params-flow>=0.8.0->bert-for-tf2) (0.4.6)\n",
      "Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n",
      "  Building wheel for bert-for-tf2 (setup.py): started\n",
      "  Building wheel for bert-for-tf2 (setup.py): finished with status 'done'\n",
      "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30557 sha256=9c6702cac0445ee3e3c8ef21e7a33993aaf87af97682e10ea660958a5bf16b44\n",
      "  Stored in directory: c:\\users\\thakr\\appdata\\local\\pip\\cache\\wheels\\d3\\b3\\5f\\1ba04fd4e099213b9c9a3f5b1ac5548fa2e599cb8fc9a868cc\n",
      "  Building wheel for params-flow (setup.py): started\n",
      "  Building wheel for params-flow (setup.py): finished with status 'done'\n",
      "  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19494 sha256=8a8680ae48eb3bb48ea48818aa38f064f564a80f645ebd8e09a06be66e3f1c7f\n",
      "  Stored in directory: c:\\users\\thakr\\appdata\\local\\pip\\cache\\wheels\\91\\17\\7a\\d8dc86bae260c349990d0c36b60685b2c4cd601866d083d471\n",
      "  Building wheel for py-params (setup.py): started\n",
      "  Building wheel for py-params (setup.py): finished with status 'done'\n",
      "  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7915 sha256=be24536f733cc6b2b70cb1995f4b3a12de59da502f55a84e9789828ba5007fe1\n",
      "  Stored in directory: c:\\users\\thakr\\appdata\\local\\pip\\cache\\wheels\\c4\\94\\74\\551bca501c76c25b5f8512e8dc531f9a24dd221cdbbd3e6321\n",
      "Successfully built bert-for-tf2 params-flow py-params\n",
      "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
      "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fc8b3b8-fa18-4c98-8071-65d5d7b892d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-text\n",
      "  Downloading tensorflow_text-2.10.0-cp37-cp37m-win_amd64.whl (5.0 MB)\n",
      "     ---------------------------------------- 5.0/5.0 MB 2.1 MB/s eta 0:00:00\n",
      "Collecting tensorflow<2.11,>=2.10.0\n",
      "  Downloading tensorflow-2.10.1-cp37-cp37m-win_amd64.whl (455.9 MB)\n",
      "     -------------------------------------- 455.9/455.9 MB 2.2 MB/s eta 0:00:00\n",
      "Collecting tensorflow-hub>=0.8.0\n",
      "  Downloading tensorflow_hub-0.13.0-py2.py3-none-any.whl (100 kB)\n",
      "     -------------------------------------- 100.6/100.6 kB 6.0 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB 3.5 MB/s eta 0:00:00\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "     ---------------------------------------- 5.9/5.9 MB 5.5 MB/s eta 0:00:00\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp37-cp37m-win_amd64.whl (896 kB)\n",
      "     -------------------------------------- 896.6/896.6 kB 5.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.16.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.54.0-cp37-cp37m-win_amd64.whl (4.1 MB)\n",
      "     ---------------------------------------- 4.1/4.1 MB 4.4 MB/s eta 0:00:00\n",
      "Collecting keras<2.11,>=2.10.0\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 4.9 MB/s eta 0:00:00\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "     -------------------------------------- 438.7/438.7 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.21.5)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp37-cp37m-win_amd64.whl (35 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.8.0-cp37-cp37m-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 4.9 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "     ---------------------------------------- 24.4/24.4 MB 5.0 MB/s eta 0:00:00\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "     -------------------------------------- 42.6/42.6 kB 703.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (4.3.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp37-cp37m-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (65.6.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (22.0)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     -------------------------------------- 126.5/126.5 kB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.11,>=2.10.0->tensorflow-text) (0.38.4)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 93.9/93.9 kB 5.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2.28.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     -------------------------------------- 781.3/781.3 kB 4.9 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
      "     -------------------------------------- 178.2/178.2 kB 3.6 MB/s eta 0:00:00\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "     -------------------------------------- 233.6/233.6 kB 4.8 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.3/181.3 kB 5.3 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (4.11.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (1.26.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (3.11.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 83.9/83.9 kB 4.9 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, keras, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, oauthlib, keras-preprocessing, h5py, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, tensorflow-hub, rsa, requests-oauthlib, pyasn1-modules, markdown, google-auth, google-auth-oauthlib, tensorboard, tensorflow, tensorflow-text\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 flatbuffers-23.3.3 gast-0.4.0 google-auth-2.17.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.54.0 h5py-3.8.0 keras-2.10.0 keras-preprocessing-1.1.2 libclang-16.0.0 markdown-3.4.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.1 tensorflow-estimator-2.10.0 tensorflow-hub-0.13.0 tensorflow-io-gcs-filesystem-0.31.0 tensorflow-text-2.10.0 termcolor-2.3.0 werkzeug-2.2.3 wrapt-1.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55ac876a-60bf-4a68-9a0b-cdf5425e8683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.8.2-py3-none-any.whl (5.3 MB)\n",
      "     ---------------------------------------- 5.3/5.3 MB 5.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: termcolor in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from tensorflow-datasets) (2.3.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from tensorflow-datasets) (5.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from tensorflow-datasets) (1.15.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from tensorflow-datasets) (5.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from tensorflow-datasets) (1.21.5)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.12.0-py3-none-any.whl (52 kB)\n",
      "     ---------------------------------------- 52.3/52.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from tensorflow-datasets) (4.65.0)\n",
      "Collecting etils[enp,epath]>=0.9.0\n",
      "  Downloading etils-0.9.0-py3-none-any.whl (140 kB)\n",
      "     -------------------------------------- 140.1/140.1 kB 8.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from tensorflow-datasets) (4.3.0)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from tensorflow-datasets) (1.4.0)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from tensorflow-datasets) (3.19.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from tensorflow-datasets) (2.28.1)\n",
      "Requirement already satisfied: toml in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from tensorflow-datasets) (0.10.2)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.8-cp37-cp37m-win_amd64.whl (102 kB)\n",
      "     -------------------------------------- 102.1/102.1 kB 5.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from tensorflow-datasets) (8.1.3)\n",
      "Requirement already satisfied: zipp in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (3.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2022.12.7)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from click->tensorflow-datasets) (4.11.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from click->tensorflow-datasets) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\thakr\\anaconda3\\envs\\pytorch_p37_2\\lib\\site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n",
      "     ------------------------------------- 223.6/223.6 kB 13.3 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21548 sha256=255bb71afd0577df37a70721d5a66a827c7c8191722ea0106d1463eed15a025e\n",
      "  Stored in directory: c:\\users\\thakr\\appdata\\local\\pip\\cache\\wheels\\9d\\ad\\15\\e6d5c43a0f01b88ee5883bd249a18e09d72821e43b1c3e8187\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, promise, googleapis-common-protos, etils, dill, tensorflow-metadata, tensorflow-datasets\n",
      "Successfully installed dill-0.3.6 dm-tree-0.1.8 etils-0.9.0 googleapis-common-protos-1.59.0 promise-2.3 tensorflow-datasets-4.8.2 tensorflow-metadata-1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ddabf9-8674-4d1a-81ad-cda7bb33f432",
   "metadata": {},
   "source": [
    "## **MuRIL model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91995a6-15ea-41fc-9c54-6d605613f789",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### ***Prerequisites***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f0e85ee-b42f-400f-9ebe-ddb8ef33c67c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from bert import bert_tokenization\n",
    "import numpy as np\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc8726c4-857c-49ee-857f-ee1e8dc78d32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(model_url, max_seq_length):\n",
    "    inputs = dict(\n",
    "        input_word_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
    "        input_mask=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
    "        input_type_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
    "    )\n",
    "\n",
    "    muril_layer = hub.KerasLayer(model_url, trainable=True)\n",
    "    outputs = muril_layer(inputs)\n",
    "\n",
    "    assert 'sequence_output' in outputs\n",
    "    assert 'pooled_output' in outputs\n",
    "    assert 'encoder_outputs' in outputs\n",
    "    assert 'default' in outputs\n",
    "    return tf.keras.Model(inputs=inputs,outputs=outputs[\"pooled_output\"]), muril_layer\n",
    "     \n",
    "\n",
    "max_seq_length = 128\n",
    "muril_model, muril_layer = get_model(\n",
    "model_url=\"https://tfhub.dev/google/MuRIL/1\", max_seq_length=max_seq_length)\n",
    "     \n",
    "\n",
    "vocab_file = muril_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = muril_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "\n",
    "def create_input(input_strings, tokenizer, max_seq_length):\n",
    "    input_ids_all, input_mask_all, input_type_ids_all = [], [], []\n",
    "    for input_string in input_strings:\n",
    "        input_tokens = [\"[CLS]\"] + tokenizer.tokenize(input_string) + [\"[SEP]\"]\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "        sequence_length = min(len(input_ids), max_seq_length)\n",
    "    \n",
    "        if len(input_ids) >= max_seq_length:\n",
    "            input_ids = input_ids[:max_seq_length]\n",
    "        else:\n",
    "            input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n",
    "\n",
    "        input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\n",
    "\n",
    "        input_ids_all.append(input_ids)\n",
    "        input_mask_all.append(input_mask)\n",
    "        input_type_ids_all.append([0] * max_seq_length)\n",
    "  \n",
    "    return np.array(input_ids_all), np.array(input_mask_all), np.array(input_type_ids_all)\n",
    "     \n",
    "\n",
    "def encode(input_text):\n",
    "    input_ids, input_mask, input_type_ids = create_input(input_text, \n",
    "                                                       tokenizer, \n",
    "                                                       max_seq_length)\n",
    "    inputs = dict(\n",
    "        input_word_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        input_type_ids=input_type_ids,\n",
    "    )\n",
    "    return muril_model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de262fd5-4d6b-4af1-bd5c-1e5de82c91c5",
   "metadata": {},
   "source": [
    "#### ***Examples***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f107ef8-bb71-4bf9-b172-88e547eb93ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between खेल & पेड़ is 0.012261751107871532\n",
      "Distance between पेड़ & पत्ते is 0.012261751107871532\n",
      "Distance between पत्ते & नीचे is 0.012261751107871532\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"खेल\", \"पेड़\",\"पत्ते\",\"नीचे\",\"बैठना\"]\n",
    "embeddings = encode(sentences)\n",
    "\n",
    "     \n",
    "dst_1 = distance.euclidean(np.array(embeddings[0]), \n",
    "                           np.array(embeddings[1]))\n",
    "print(\"Distance between {} & {} is {}\".format(sentences[0],\n",
    "                                                sentences[1],\n",
    "                                                dst_1))\n",
    "\n",
    "dst_2 = distance.euclidean(np.array(embeddings[1]), \n",
    "                           np.array(embeddings[2]))\n",
    "print(\"Distance between {} & {} is {}\".format(sentences[1],\n",
    "                                                sentences[2],\n",
    "                                                dst_1))\n",
    "\n",
    "dst_2 = distance.euclidean(np.array(embeddings[2]), \n",
    "                           np.array(embeddings[3]))\n",
    "print(\"Distance between {} & {} is {}\".format(sentences[2],\n",
    "                                                sentences[3],\n",
    "                                                dst_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece753fe-37d4-4476-ad13-848cb31ca201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between पेड़ & ताश is 0.0185878686606884\n",
      "Distance between ताश & खेल is 0.01976960524916649\n"
     ]
    }
   ],
   "source": [
    "sentences=[\"पेड़\", \"ताश\", \"खेल\"]\n",
    "embeddings = encode(sentences)\n",
    "     \n",
    "\n",
    "dst_1 = distance.euclidean(np.array(embeddings[0]), \n",
    "                           np.array(embeddings[1]))\n",
    "print(\"Distance between {} & {} is {}\".format(sentences[0],\n",
    "                                                sentences[1],\n",
    "                                                dst_1))\n",
    "\n",
    "dst_2 = distance.euclidean(np.array(embeddings[1]), \n",
    "                           np.array(embeddings[2]))\n",
    "print(\"Distance between {} & {} is {}\".format(sentences[1],\n",
    "                                                sentences[2],\n",
    "                                                dst_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd6d06bb-f6e3-4a54-8ac9-0b3077c9c8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between पेड़ & शिखा is 0.013731339015066624\n",
      "Distance between शिखा & खेल is 0.015045817010104656\n"
     ]
    }
   ],
   "source": [
    "sentences=[\"पेड़\", \"शिखा\", \"खेल\"]\n",
    "embeddings = encode(sentences)\n",
    "\n",
    "\n",
    "dst_1 = distance.euclidean(np.array(embeddings[0]), \n",
    "                           np.array(embeddings[1]))\n",
    "print(\"Distance between {} & {} is {}\".format(sentences[0],\n",
    "                                                sentences[1],\n",
    "                                                dst_1))\n",
    "\n",
    "dst_2 = distance.euclidean(np.array(embeddings[1]), \n",
    "                           np.array(embeddings[2]))\n",
    "print(\"Distance between {} & {} is {}\".format(sentences[1],\n",
    "                                                sentences[2],\n",
    "                                                dst_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d3f6680-f8bf-46fd-8cb5-7c14ecbbcf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between पेड़ & चादर is 0.010924091562628746\n",
      "Distance between चादर & खेल is 0.01224229484796524\n"
     ]
    }
   ],
   "source": [
    "sentences=[\"पेड़\", \"चादर\", \"खेल\"]\n",
    "embeddings = encode(sentences)\n",
    "     \n",
    "\n",
    "dst_1 = distance.euclidean(np.array(embeddings[0]), \n",
    "                           np.array(embeddings[1]))\n",
    "print(\"Distance between {} & {} is {}\".format(sentences[0],\n",
    "                                                sentences[1],\n",
    "                                                dst_1))\n",
    "\n",
    "dst_2 = distance.euclidean(np.array(embeddings[1]), \n",
    "                           np.array(embeddings[2]))\n",
    "print(\"Distance between {} & {} is {}\".format(sentences[1],\n",
    "                                                sentences[2],\n",
    "                                                dst_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b52695-e6ea-4f60-9656-1923b12881a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
